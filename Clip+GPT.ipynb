{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom transformers import CLIPProcessor, CLIPModel, GPT2Tokenizer, GPT2LMHeadModel\nimport pandas as pd\nimport os\nfrom PIL import Image\nimport numpy as np\nimport nltk\n\nnltk.download(\"punkt\")\n\n# Configuration\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\nEPOCHS = 15\nLEARNING_RATE = 5e-5\nSEED = 42\n\ntorch.manual_seed(SEED)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:07:12.021650Z","iopub.execute_input":"2024-12-17T23:07:12.022621Z","iopub.status.idle":"2024-12-17T23:07:29.805314Z","shell.execute_reply.started":"2024-12-17T23:07:12.022583Z","shell.execute_reply":"2024-12-17T23:07:29.804376Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x783710d2ffd0>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\n# Function to load captions into a DataFrame\ndef load_captions_to_df(captions_file):\n    \"\"\"\n    Load the captions file into a Pandas DataFrame.\n    Args:\n        captions_file: Path to the Flickr8k captions text file.\n    Returns:\n        DataFrame with two columns: 'image_id' and 'caption'.\n    \"\"\"\n    df = pd.read_csv(captions_file, sep=',')\n    return df\n\n# Load the captions\nCAPTIONS_FILE = \"/kaggle/input/flickr8k/captions.txt\"\nIMAGE_FOLDER = \"/kaggle/input/flickr8k/Images\"\ndf_captions = load_captions_to_df(CAPTIONS_FILE)\n\nprint(df_captions.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:07:29.807227Z","iopub.execute_input":"2024-12-17T23:07:29.808641Z","iopub.status.idle":"2024-12-17T23:07:29.914790Z","shell.execute_reply.started":"2024-12-17T23:07:29.808598Z","shell.execute_reply":"2024-12-17T23:07:29.913932Z"}},"outputs":[{"name":"stdout","text":"                       image  \\\n0  1000268201_693b08cb0e.jpg   \n1  1000268201_693b08cb0e.jpg   \n2  1000268201_693b08cb0e.jpg   \n3  1000268201_693b08cb0e.jpg   \n4  1000268201_693b08cb0e.jpg   \n\n                                             caption  \n0  A child in a pink dress is climbing up a set o...  \n1              A girl going into a wooden building .  \n2   A little girl climbing into a wooden playhouse .  \n3  A little girl climbing the stairs to her playh...  \n4  A little girl in a pink dress going into a woo...  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class Flickr8kDataset(Dataset):\n    def __init__(self, image_folder, captions_df, processor, tokenizer, transform=None):\n        self.image_folder = image_folder\n        self.captions_df = captions_df\n        self.processor = processor\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n        # Group captions by image ID\n        self.image_to_captions = captions_df.groupby(\"image\")[\"caption\"].apply(list).to_dict()\n        self.image_files = list(self.image_to_captions.keys())\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        image_id = self.image_files[idx]\n        captions = self.image_to_captions[image_id]\n\n        # Load and preprocess image\n        image_path = os.path.join(self.image_folder, image_id)\n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n\n        # CLIP processor\n        image_inputs = self.processor(images=image, return_tensors=\"pt\", padding=True)\n        image_inputs = {k: v.squeeze(0) for k, v in image_inputs.items()}\n\n        # Randomly select a caption\n        caption = np.random.choice(captions)\n        caption_tokens = self.tokenizer.encode(caption, return_tensors=\"pt\").squeeze(0)\n\n        return image_inputs, caption_tokens\n\ndef collate_fn(batch):\n    image_inputs = [item[0] for item in batch]\n    captions = [item[1] for item in batch]\n\n    max_len = max(len(cap) for cap in captions)\n    padded_captions = torch.full((len(captions), max_len), gpt_tokenizer.pad_token_id, dtype=torch.long)\n\n    for i, cap in enumerate(captions):\n        padded_captions[i, :len(cap)] = cap\n\n    batch_image_inputs = {}\n    for key in image_inputs[0].keys():\n        batch_image_inputs[key] = torch.stack([img[key] for img in image_inputs])\n\n    return batch_image_inputs, padded_captions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:07:29.915818Z","iopub.execute_input":"2024-12-17T23:07:29.916074Z","iopub.status.idle":"2024-12-17T23:07:29.926038Z","shell.execute_reply.started":"2024-12-17T23:07:29.916050Z","shell.execute_reply":"2024-12-17T23:07:29.925172Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class ImageCaptioningModel(nn.Module):\n    def __init__(self, clip_model_name=\"openai/clip-vit-base-patch32\", gpt_model_name=\"gpt2\"):\n        super(ImageCaptioningModel, self).__init__()\n        self.clip_model = CLIPModel.from_pretrained(clip_model_name).vision_model\n        self.gpt_model = GPT2LMHeadModel.from_pretrained(gpt_model_name)\n        self.clip_to_gpt = nn.Linear(768, self.gpt_model.config.hidden_size)\n\n    def forward(self, image_inputs, captions):\n        # Extract CLIP image embeddings\n        image_features = self.clip_model(**image_inputs).pooler_output\n        image_features = self.clip_to_gpt(image_features).unsqueeze(1)\n\n        # Embed captions\n        caption_embeddings = self.gpt_model.transformer.wte(captions)\n\n        # Combine image and caption embeddings\n        inputs_embeds = torch.cat([image_features, caption_embeddings], dim=1)\n        outputs = self.gpt_model(inputs_embeds=inputs_embeds)\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:07:29.927780Z","iopub.execute_input":"2024-12-17T23:07:29.928046Z","iopub.status.idle":"2024-12-17T23:07:29.942054Z","shell.execute_reply.started":"2024-12-17T23:07:29.928021Z","shell.execute_reply":"2024-12-17T23:07:29.941416Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Initialize Processor, Tokenizer, and Dataset\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\ngpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ngpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\ndataset = Flickr8kDataset(IMAGE_FOLDER, df_captions, clip_processor, gpt_tokenizer)\ntrain_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n\n# Initialize Model, Optimizer, and Criterion\nmodel = ImageCaptioningModel().to(DEVICE)\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss(ignore_index=gpt_tokenizer.pad_token_id)\n\n# Training Loop\ndef train(model, dataloader, optimizer, criterion, epoch):\n    model.train()\n    total_loss = 0\n\n    for batch in dataloader:\n        image_inputs, captions = batch\n        image_inputs = {k: v.to(DEVICE) for k, v in image_inputs.items()}\n        captions = captions.to(DEVICE)\n\n        optimizer.zero_grad()\n        outputs = model(image_inputs, captions)\n\n        # Get logits (model predictions)\n        logits = outputs.logits  # Shape: [batch_size, sequence_length + 1, vocab_size]\n\n        # Shift labels to the right\n        labels = captions[:, 1:]  # Drop the first token (start token)\n\n        # Trim logits to align with labels\n        logits = logits[:, :labels.size(1), :]  # Match logits to labels' length\n\n        # Flatten logits and labels for loss calculation\n        logits = logits.reshape(-1, logits.size(-1))  # Shape: (batch_size * sequence_length, vocab_size)\n        labels = labels.reshape(-1)  # Shape: (batch_size * sequence_length)\n\n        # Compute loss, ignoring padding tokens\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    average_loss = total_loss / len(dataloader)\n    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {average_loss:.4f}\")\n\n    # Save the model at the end of the epoch\n    model_save_path = f\"image_captioning_model_epoch_{epoch+1}.pth\"\n    torch.save(model.state_dict(), model_save_path)\n    print(f\"Model saved at: {model_save_path}\")\n\n    return average_loss\n\n\n# Run Training and Save the Model\nprint(\"Training Begins\")\nfor epoch in range(EPOCHS):\n    \n    train_loss = train(model, train_loader, optimizer, criterion, epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:07:29.943228Z","iopub.execute_input":"2024-12-17T23:07:29.943699Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96c3adca46f1464db03538f1ab9e9e6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72a548a27f7b496fb3bf2002da096d36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3062c010eb7843449c2d865438b8f5c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11dd4f37d3f84a7984d04643db65b32d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4c4778731a44239bad45233cc0980e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b94ef25d28149118b80e571e8d19278"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e693c5669d94e0fad05d1a7c84ec592"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52b8276c0d934b6597c1f464e716b981"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c0ca2047cc44331850997502aaba38f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"669c34113acf4ea0a575b3b086572ddc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a15ff6a974c74f0ab2fb0322be48f6b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3032024f660e4824bf5dd639bc857008"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89981af08b2c43d3aa4f5f2154316593"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6701f6e22bb48b1a4e86f03556ada02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39ab40b4d4f4437781ff44dc661f82bc"}},"metadata":{}},{"name":"stdout","text":"Training Begins\nEpoch [1/15], Loss: 4.8281\nModel saved at: image_captioning_model_epoch_1.pth\nEpoch [2/15], Loss: 4.3918\nModel saved at: image_captioning_model_epoch_2.pth\nEpoch [3/15], Loss: 4.2014\nModel saved at: image_captioning_model_epoch_3.pth\nEpoch [4/15], Loss: 4.0489\nModel saved at: image_captioning_model_epoch_4.pth\nEpoch [5/15], Loss: 3.9484\nModel saved at: image_captioning_model_epoch_5.pth\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def generate_caption(model, image_path, processor, tokenizer, max_length=20):\n    model.eval()\n    image = Image.open(image_path).convert(\"RGB\")\n    inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n\n    with torch.no_grad():\n        image_features = model.clip_model(**inputs).pooler_output\n        image_features = model.clip_to_gpt(image_features).unsqueeze(1)\n\n        generated = torch.tensor([[tokenizer.bos_token_id]], device=DEVICE)\n        for _ in range(max_length):\n            gpt_input = torch.cat([image_features, model.gpt_model.transformer.wte(generated)], dim=1)\n            outputs = model.gpt_model(inputs_embeds=gpt_input)\n            next_token = torch.argmax(outputs.logits[:, -1, :], dim=-1)\n            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n\n    return tokenizer.decode(generated.squeeze(), skip_special_tokens=True)\n\n# Test\ntest_image_path = \"/kaggle/input/flickr8k/Images/1001773457_577c3a7d70.jpg\"\ncaption = generate_caption(model, test_image_path, clip_processor, gpt_tokenizer)\nprint(\"Generated Caption:\", caption)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}