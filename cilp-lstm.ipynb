{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom PIL import Image\n\n!pip install git+https://github.com/openai/CLIP.git\n\nimport clip\nimport numpy as np","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-12-18T00:36:04.885793Z","iopub.execute_input":"2024-12-18T00:36:04.886633Z","iopub.status.idle":"2024-12-18T00:36:21.850564Z","shell.execute_reply.started":"2024-12-18T00:36:04.886595Z","shell.execute_reply":"2024-12-18T00:36:21.849670Z"},"id":"1Gc-D0P0_kn1","outputId":"19622d27-42a7-4fd5-c74f-926f2b8a04ee","trusted":true},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-_nr8oo9j\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-_nr8oo9j\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2024.5.15)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.19.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->clip==1.0) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2024.6.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=a534a69350096a7a0194571395d60e2c1b08af85defa4bf6a11fb13488320f71\n  Stored in directory: /tmp/pip-ephem-wheel-cache-hsvmphjv/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.3.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Load captions\ndef load_captions(captions_file):\n    captions = []\n    with open(captions_file, 'r') as f:\n        for line in f:\n            img, caption = line.strip().split(',', 1)  # Split only on the first comma\n            if (img != 'image'):\n                captions.append((img.strip(), caption.strip()))\n    return captions\n\n\n# Preprocess images using CLIP\ndef preprocess_images(image_folder, clip_preprocess, device):\n    images = {}\n    for img_name in os.listdir(image_folder):\n        try:\n            img_path = os.path.join(image_folder, img_name)\n            image = Image.open(img_path).convert(\"RGB\")\n            image_input = clip_preprocess(image).unsqueeze(0).to(device).to(torch.float32)\n\n            # image_input = clip_preprocess(image).unsqueeze(0).to(device)\n            images[img_name] = image_input\n        except Exception as e:\n            print(f\"Error processing {img_name}: {e}\")\n    return images\n\n# Tokenizer function\n# def tokenize_caption(caption, vocab, max_len=20):\n#     tokens = caption.lower().split()[:max_len]\n#     return [vocab.get(token, vocab['<unk>']) for token in tokens] + [0] * (max_len - len(tokens))\n\n# def tokenize_caption(caption, vocab, max_len=20):\n#     tokens = caption.lower().split()\n#     token_indices = [vocab.get(word, vocab[\"<unk>\"]) for word in tokens]\n#     token_indices = [vocab[\"start\"]] + token_indices[:max_len - 2] + [vocab[\"end\"]]\n#     token_indices += [vocab[\"<pad>\"]] * (max_len - len(token_indices))\n#     return token_indices\n\ndef tokenize_caption(caption, vocab, max_len=20):\n    words = caption.lower().split()\n    \n    # Convert words to indices and include start/end tokens\n    tokens = [vocab['<start>']] + [vocab.get(w, vocab['<unk>']) for w in words] + [vocab['<end>']]\n\n    # If the sequence is longer than max_len, truncate and ensure last token is <end>\n    if len(tokens) > max_len:\n        tokens = tokens[:max_len]\n        tokens[-1] = vocab['<end>']\n\n    # If shorter, pad with <pad>\n    if len(tokens) < max_len:\n        tokens += [vocab['<pad>']] * (max_len - len(tokens))\n\n    return tokens\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T00:36:21.852494Z","iopub.execute_input":"2024-12-18T00:36:21.852979Z","iopub.status.idle":"2024-12-18T00:36:21.862166Z","shell.execute_reply.started":"2024-12-18T00:36:21.852952Z","shell.execute_reply":"2024-12-18T00:36:21.861465Z"},"id":"qPc-XD8g_sKD","trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass FlickrDataset(Dataset):\n    def __init__(self, captions, images, vocab):\n        self.captions = [(img, cap) for img, cap in captions if img in images]\n        self.images = images\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.captions)\n\n    def __getitem__(self, idx):\n        img_name, caption = self.captions[idx]\n        if img_name not in self.images:\n            raise ValueError(f\"Image {img_name} not found in preprocessed images!\")\n        \n        return img_name, torch.tensor(tokenize_caption(caption, self.vocab))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"execution":{"iopub.status.busy":"2024-12-18T00:36:21.863115Z","iopub.execute_input":"2024-12-18T00:36:21.863371Z","iopub.status.idle":"2024-12-18T00:36:21.875672Z","shell.execute_reply.started":"2024-12-18T00:36:21.863329Z","shell.execute_reply":"2024-12-18T00:36:21.874926Z"},"id":"5FG_dsa-Ax2p","outputId":"2249fc7e-f270-45f7-b5ea-540496d37541","trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef split_data(captions, test_size=0.2, random_state=42):\n\n    img_names = list(set([img for img, _ in captions]))  # Unique image names\n    train_imgs, test_imgs = train_test_split(img_names, test_size=test_size, random_state=random_state)\n\n    train_captions = [(img, cap) for img, cap in captions if img in train_imgs]\n    test_captions = [(img, cap) for img, cap in captions if img in test_imgs]\n\n    return train_captions, test_captions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T00:36:21.877740Z","iopub.execute_input":"2024-12-18T00:36:21.878025Z","iopub.status.idle":"2024-12-18T00:36:22.521165Z","shell.execute_reply.started":"2024-12-18T00:36:21.878000Z","shell.execute_reply":"2024-12-18T00:36:22.520452Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class CLIPEncoder(nn.Module):\n    def __init__(self, device):\n        super(CLIPEncoder, self).__init__()\n        self.device = device\n        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n\n    def forward(self, image):\n        with torch.no_grad():\n            features = self.model.encode_image(image)\n            features = features / features.norm(dim=-1, keepdim=True)  # Normalize\n        return features.unsqueeze(1)  # Shape: (batch_size, 1, 512)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T00:36:22.522149Z","iopub.execute_input":"2024-12-18T00:36:22.522530Z","iopub.status.idle":"2024-12-18T00:36:22.528592Z","shell.execute_reply.started":"2024-12-18T00:36:22.522501Z","shell.execute_reply":"2024-12-18T00:36:22.527664Z"},"id":"4jCfzqeiAzuv","trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, encoder_dim):\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, 512)\n        self.decoder_att = nn.Linear(512, 512)\n        self.full_att = nn.Linear(512, 1)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, encoder_out, decoder_hidden):\n        att1 = self.encoder_att(encoder_out)\n        att2 = self.decoder_att(decoder_hidden)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n        alpha = self.softmax(att)\n        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n        return context, alpha\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T00:36:22.529755Z","iopub.execute_input":"2024-12-18T00:36:22.530101Z","iopub.status.idle":"2024-12-18T00:36:22.543901Z","shell.execute_reply.started":"2024-12-18T00:36:22.530063Z","shell.execute_reply":"2024-12-18T00:36:22.543182Z"},"id":"hqDAjxopA3Qp","trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, vocabulary_size, encoder_dim, tf=False):\n        super(Decoder, self).__init__()\n        self.use_tf = tf\n        self.vocabulary_size = vocabulary_size\n        self.encoder_dim = encoder_dim\n\n        self.init_h = nn.Linear(encoder_dim, 512)\n        self.init_c = nn.Linear(encoder_dim, 512)\n        self.tanh = nn.Tanh()\n\n        self.f_beta = nn.Linear(512, encoder_dim)\n        self.sigmoid = nn.Sigmoid()\n\n        self.deep_output = nn.Linear(512, vocabulary_size)\n        self.dropout = nn.Dropout()\n\n        self.attention = Attention(encoder_dim)\n        self.embedding = nn.Embedding(vocabulary_size, 512)\n        self.lstm = nn.LSTMCell(512 + encoder_dim, 512)\n\n    def forward(self, img_features, captions):\n        batch_size = img_features.size(0)\n        max_timespan = captions.size(1) - 1\n\n        h, c = self.get_init_lstm_state(img_features)\n\n        embedding = self.embedding(captions)\n\n        preds = torch.zeros(batch_size, max_timespan, self.vocabulary_size).to(img_features.device)\n        alphas = torch.zeros(batch_size, max_timespan, img_features.size(1)).to(img_features.device)\n\n        for t in range(max_timespan):\n            context, alpha = self.attention(img_features, h)\n            gate = self.sigmoid(self.f_beta(h))\n            gated_context = gate * context\n\n            lstm_input = torch.cat((embedding[:, t], gated_context), dim=1)\n            h, c = self.lstm(lstm_input, (h, c))\n            preds[:, t] = self.deep_output(self.dropout(h))\n            alphas[:, t] = alpha\n\n        return preds, alphas\n\n    def get_init_lstm_state(self, img_features):\n        img_features = img_features.to(torch.float32)\n        avg_features = img_features.mean(dim=1)\n        c = self.tanh(self.init_c(avg_features))\n        h = self.tanh(self.init_h(avg_features))\n        return h, c\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T00:36:22.544882Z","iopub.execute_input":"2024-12-18T00:36:22.545211Z","iopub.status.idle":"2024-12-18T00:36:22.556265Z","shell.execute_reply.started":"2024-12-18T00:36:22.545175Z","shell.execute_reply":"2024-12-18T00:36:22.555644Z"},"id":"R1GgPx3-A464","trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class EncoderDecoder(nn.Module):\n    def __init__(self, vocab_size, device):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = CLIPEncoder(device)\n        self.decoder = Decoder(vocab_size, encoder_dim=512)\n\n    def forward(self, images, captions):\n        encoder_out = self.encoder(images)\n        preds, alphas = self.decoder(encoder_out, captions)\n        return preds, alphas\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T00:36:22.557435Z","iopub.execute_input":"2024-12-18T00:36:22.557744Z","iopub.status.idle":"2024-12-18T00:36:22.569889Z","shell.execute_reply.started":"2024-12-18T00:36:22.557719Z","shell.execute_reply":"2024-12-18T00:36:22.569212Z"},"id":"rRWySoCgA7OA","trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def train_model(model, data_loader, criterion, optimizer, vocab_size, device, num_epochs=10):\n    \n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        for img_names, captions in data_loader:\n            # Filter out invalid keys (e.g., 'image' or missing images)\n            valid_images = []\n            for name in img_names:\n                if name in preprocessed_images:\n                    valid_images.append(preprocessed_images[name])\n                else:\n                    print(f\"Skipping invalid key: {name}\")\n\n            if not valid_images:\n                continue  # Skip batch if no valid images\n\n            # Combine valid images into a tensor\n            images = torch.cat(valid_images).to(device)\n            # print(type(captions))\n            captions = captions.to(device)\n\n\n            # Forward pass\n            preds, alphas = model(images, captions)\n\n            # Align preds and targets\n            max_seq_len = captions.size(1) - 1  # Exclude <end> token\n            preds = preds[:, :max_seq_len, :].contiguous()\n            targets = captions[:, 1:max_seq_len + 1].contiguous()\n\n            # Debugging: Print shapes to ensure alignment\n            # print(f\"Preds shape: {preds.shape}, Targets shape: {targets.shape}\")\n\n            # Compute loss\n            loss = criterion(preds.view(-1, vocab_size), targets.view(-1))\n\n            # Backpropagation\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(data_loader)}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T00:36:22.570836Z","iopub.execute_input":"2024-12-18T00:36:22.571102Z","iopub.status.idle":"2024-12-18T00:36:22.582001Z","shell.execute_reply.started":"2024-12-18T00:36:22.571075Z","shell.execute_reply":"2024-12-18T00:36:22.581327Z"},"id":"-C6q_Sh_A9Nr","trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# vocab = {\"<pad>\": 0, \"<unk>\": 1, \"start\": 2, \"end\": 3}  # Example vocabulary\n# vocab_size = len(vocab)\n\n\ncaptions_file = \"/kaggle/input/captions.txt\"\nimage_folder = \"/kaggle/input/Images\"\n\n# Load captions\ncaptions = load_captions(captions_file)\n\n# Preprocess images\npreprocessed_images = preprocess_images(image_folder, clip.load(\"ViT-B/32\")[1], device)","metadata":{"execution":{"iopub.status.busy":"2024-12-18T00:36:22.584143Z","iopub.execute_input":"2024-12-18T00:36:22.584456Z","iopub.status.idle":"2024-12-18T00:38:16.457266Z","shell.execute_reply.started":"2024-12-18T00:36:22.584410Z","shell.execute_reply":"2024-12-18T00:38:16.456340Z"},"id":"dfJ9968HA_I3","trusted":true},"outputs":[{"name":"stderr","text":"100%|████████████████████████████████████████| 338M/338M [00:01<00:00, 193MiB/s]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from collections import Counter\n\ndef build_vocab(captions, min_freq=1):\n    \"\"\"\n    Build vocabulary from captions.\n    :param captions: List of (img_name, caption) pairs\n    :param min_freq: Minimum frequency for a word to be included\n    :return: vocab (word to index mapping)\n    \"\"\"\n    word_counts = Counter()\n\n    # Tokenize and count words\n    for _, caption in captions:\n        words = caption.lower().split()  # Simple split by spaces\n        word_counts.update(words)\n    \n    # Create the vocabulary with special tokens\n    vocab = {\"<pad>\": 0, \"<unk>\": 1, \"<start>\": 2, \"<end>\": 3}\n    index = 4\n\n    # Add words to the vocabulary that meet the frequency threshold\n    for word, count in word_counts.items():\n        if count >= min_freq:\n            vocab[word] = index\n            index += 1\n\n    return vocab\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T00:38:16.458613Z","iopub.execute_input":"2024-12-18T00:38:16.459207Z","iopub.status.idle":"2024-12-18T00:38:16.465461Z","shell.execute_reply.started":"2024-12-18T00:38:16.459163Z","shell.execute_reply":"2024-12-18T00:38:16.464533Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"vocab = build_vocab(captions, min_freq=1)\nprint(f\"Vocabulary size: {len(vocab)}\")\nprint(f\"Sample words: {list(vocab.keys())[:10]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T00:38:16.466727Z","iopub.execute_input":"2024-12-18T00:38:16.467064Z","iopub.status.idle":"2024-12-18T00:38:16.575165Z","shell.execute_reply.started":"2024-12-18T00:38:16.467026Z","shell.execute_reply":"2024-12-18T00:38:16.574426Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 9184\nSample words: ['<pad>', '<unk>', '<start>', '<end>', 'a', 'child', 'in', 'pink', 'dress', 'is']\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Split the captions\ntrain_captions, test_captions = split_data(captions, test_size=0.2)\n\n# Create datasets\ntrain_dataset = FlickrDataset(train_captions, preprocessed_images, vocab)\ntest_dataset = FlickrDataset(test_captions, preprocessed_images, vocab)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nprint(f\"Training samples: {len(train_dataset)}, Testing samples: {len(test_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T00:38:16.576913Z","iopub.execute_input":"2024-12-18T00:38:16.577171Z","iopub.status.idle":"2024-12-18T00:38:20.406839Z","shell.execute_reply.started":"2024-12-18T00:38:16.577146Z","shell.execute_reply":"2024-12-18T00:38:20.405855Z"}},"outputs":[{"name":"stdout","text":"Training samples: 32360, Testing samples: 8095\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"dataset = FlickrDataset(captions, preprocessed_images, vocab)\ndata_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\nmodel = EncoderDecoder(len(vocab), device).to(device)\nmodel = model.to(torch.float32)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\nprint(\"00\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T00:38:20.407833Z","iopub.execute_input":"2024-12-18T00:38:20.408114Z","iopub.status.idle":"2024-12-18T00:38:24.960962Z","shell.execute_reply.started":"2024-12-18T00:38:20.408087Z","shell.execute_reply":"2024-12-18T00:38:24.960062Z"}},"outputs":[{"name":"stdout","text":"00\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(f\"Vocabulary size passed to Decoder: {len(vocab)}\")\nprint(f\"Decoder's vocabulary size: {model.decoder.vocabulary_size}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T00:38:24.962175Z","iopub.execute_input":"2024-12-18T00:38:24.962584Z","iopub.status.idle":"2024-12-18T00:38:24.967234Z","shell.execute_reply.started":"2024-12-18T00:38:24.962543Z","shell.execute_reply":"2024-12-18T00:38:24.966441Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size passed to Decoder: 9184\nDecoder's vocabulary size: 9184\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"train_model(model, train_loader, criterion, optimizer, len(vocab), device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T00:38:24.968342Z","iopub.execute_input":"2024-12-18T00:38:24.968714Z","iopub.status.idle":"2024-12-18T01:02:17.459409Z","shell.execute_reply.started":"2024-12-18T00:38:24.968676Z","shell.execute_reply":"2024-12-18T01:02:17.458378Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10, Loss: 4.600524904935257\nEpoch 2/10, Loss: 3.7553504078755737\nEpoch 3/10, Loss: 3.4727476170882876\nEpoch 4/10, Loss: 3.276135066045603\nEpoch 5/10, Loss: 3.12910344807998\nEpoch 6/10, Loss: 3.0113821642200937\nEpoch 7/10, Loss: 2.913134193231937\nEpoch 8/10, Loss: 2.828891282025062\nEpoch 9/10, Loss: 2.75844442703036\nEpoch 10/10, Loss: 2.6925692461696067\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def generate_captions(model, test_loader, vocab, device, max_len=20):\n    \"\"\"\n    Generate captions for test images\n    :param model: Trained Encoder-Decoder model\n    :param test_loader: DataLoader for test data\n    :param vocab: Vocabulary dictionary (word to index mapping)\n    :param device: Device to run the model (CPU or GPU)\n    :param max_len: Maximum length of generated captions\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n    idx_to_word = {v: k for k, v in vocab.items()}  # Reverse vocab to map indices to words\n\n    print(\"Generating captions on test data...\")\n    with torch.no_grad():\n        for img_names, captions in test_loader:\n            # Prepare images\n            images = torch.cat([preprocessed_images[name] for name in img_names]).to(device)\n            actual_captions = [[idx_to_word[idx.item()] for idx in caption if idx.item() != vocab[\"<pad>\"]] for caption in captions]\n\n            # Forward pass through the encoder\n            encoder_out = model.encoder(images)\n\n            # Initialize LSTM state and start token\n            h, c = model.decoder.get_init_lstm_state(encoder_out)\n            prev_word = torch.tensor([vocab['start']] * len(img_names)).to(device)  # Start tokens\n\n            # Store generated captions\n            generated_captions = [[] for _ in range(len(img_names))]\n\n            for _ in range(max_len):\n                # Embed previous word\n                embedding = model.decoder.embedding(prev_word)\n\n                # Attention\n                context, _ = model.decoder.attention(encoder_out, h)\n                gate = model.decoder.sigmoid(model.decoder.f_beta(h))\n                gated_context = gate * context\n\n                # LSTM forward\n                lstm_input = torch.cat((embedding, gated_context), dim=1)\n                h, c = model.decoder.lstm(lstm_input, (h, c))\n\n                # Predict next word\n                output = model.decoder.deep_output(h)  # (batch_size, vocab_size)\n                prev_word = output.argmax(1)  # Greedy decoding: Pick the word with max probability\n\n                # Append words to captions\n                for i in range(len(img_names)):\n                    word = idx_to_word.get(prev_word[i].item(), '<unk>')\n                    if word == 'end':  # Stop generating if 'end' token is predicted\n                        continue\n                    generated_captions[i].append(word)\n\n            # temp=0\n            # count=5\n            # # Print generated and actual captions\n            # for i, name in enumerate(img_names):\n            #     temp=temp+1\n            #     if temp < count:\n            #         print(f\"Image: {name}\")\n            #         print(f\"Generated Caption: {' '.join(generated_captions[i])}\")\n            #         print(f\"Actual Caption: {' '.join(actual_captions[i])}\")\n            #         print(\"-\" * 50)\n    return actual_captions, generated_captions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T01:05:12.821345Z","iopub.execute_input":"2024-12-18T01:05:12.821748Z","iopub.status.idle":"2024-12-18T01:05:12.831504Z","shell.execute_reply.started":"2024-12-18T01:05:12.821718Z","shell.execute_reply":"2024-12-18T01:05:12.830594Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"print(f\"Vocabulary size: {len(vocab)}\")\nprint(f\"Sample words: {list(vocab.keys())[:10]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T01:05:16.068940Z","iopub.execute_input":"2024-12-18T01:05:16.069634Z","iopub.status.idle":"2024-12-18T01:05:16.074505Z","shell.execute_reply.started":"2024-12-18T01:05:16.069597Z","shell.execute_reply":"2024-12-18T01:05:16.073601Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 9184\nSample words: ['<pad>', '<unk>', '<start>', '<end>', 'a', 'child', 'in', 'pink', 'dress', 'is']\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"actual_captions, generated_captions = generate_captions(model, test_loader, vocab, device, max_len=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T01:06:48.130200Z","iopub.execute_input":"2024-12-18T01:06:48.130580Z","iopub.status.idle":"2024-12-18T01:07:14.721056Z","shell.execute_reply.started":"2024-12-18T01:06:48.130548Z","shell.execute_reply":"2024-12-18T01:07:14.720341Z"}},"outputs":[{"name":"stdout","text":"Generating captions on test data...\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"for i in range(10):\n    print(f\"Generated Caption: {' '.join(generated_captions[i])}\")\n    print(f\"Actual Caption: {' '.join(actual_captions[i])}\")\n    print(\"---------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T01:07:53.401524Z","iopub.execute_input":"2024-12-18T01:07:53.401881Z","iopub.status.idle":"2024-12-18T01:07:53.407084Z","shell.execute_reply.started":"2024-12-18T01:07:53.401851Z","shell.execute_reply":"2024-12-18T01:07:53.406199Z"}},"outputs":[{"name":"stdout","text":"Generated Caption: of a child in a blue shirt and a blue shirt <end> <end> <end> <end> <end> <end> <end> <end> <end>\nActual Caption: <start> the young boy is pushing the cart inside the store . <end>\n---------\nGenerated Caption: man in a blue shirt and blue shorts is riding a bike <end> <end> <end> <end> <end> <end> <end> <end>\nActual Caption: <start> a man in red swim trunks is jumping onto a bodyboard . <end>\n---------\nGenerated Caption: man in a blue shirt and blue shorts is riding a bike <end> <end> <end> <end> <end> <end> <end> <end>\nActual Caption: <start> a man in red trunks flies through the air with a boogie board . <end>\n---------\nGenerated Caption: man in a blue shirt and blue shorts is riding a bike <end> <end> <end> <end> <end> <end> <end> <end>\nActual Caption: <start> a man with a wake-board is diving over a surface that is not water . <end>\n---------\nGenerated Caption: man in a blue shirt and blue shorts is riding a bike <end> <end> <end> <end> <end> <end> <end> <end>\nActual Caption: <start> a shirtless man bodysurfs . <end>\n---------\nGenerated Caption: man in a blue shirt and blue shorts is riding a bike <end> <end> <end> <end> <end> <end> <end> <end>\nActual Caption: <start> man is diving onto his wakeboard . <end>\n---------\nGenerated Caption: dog running through the snow . <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end>\nActual Caption: <start> a brown dog is running over snow near leafless trees . <end>\n---------\nGenerated Caption: dog running through the snow . <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end>\nActual Caption: <start> a brown dog runs across a snowy field . <end>\n---------\nGenerated Caption: dog running through the snow . <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end>\nActual Caption: <start> a dog is running through the snow . <end>\n---------\nGenerated Caption: dog running through the snow . <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end>\nActual Caption: <start> a dog runs through the snow . <end>\n---------\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}