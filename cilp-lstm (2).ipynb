{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom PIL import Image\n\n!pip install git+https://github.com/openai/CLIP.git\n\nimport clip\nimport numpy as np","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-12-18T15:05:26.717271Z","iopub.execute_input":"2024-12-18T15:05:26.717520Z","iopub.status.idle":"2024-12-18T15:05:45.536385Z","shell.execute_reply.started":"2024-12-18T15:05:26.717494Z","shell.execute_reply":"2024-12-18T15:05:45.535445Z"},"id":"1Gc-D0P0_kn1","outputId":"19622d27-42a7-4fd5-c74f-926f2b8a04ee","trusted":true},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ylk5qpru\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ylk5qpru\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2024.5.15)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.19.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->clip==1.0) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2024.6.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=d2306e6d3b967c7d782fb22dca9013de2b762a107b2031eadc929ed8f133d791\n  Stored in directory: /tmp/pip-ephem-wheel-cache-6ltk5ctw/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.3.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Utility Functions that load captions, preprocess images and tokenize captions ###","metadata":{}},{"cell_type":"code","source":"# Some utility functions are defined above\n\n# Load captions\ndef load_captions(captions_file):\n    captions = []\n    with open(captions_file, 'r') as f:\n        for line in f:\n            img, caption = line.strip().split(',', 1)  # Split on the first comma\n            if (img != 'image'):\n                captions.append((img.strip(), caption.strip()))\n    return captions\n\n\n# Preprocess images using CLIP\ndef preprocess_images(image_folder, clip_preprocess, device):\n    images = {}\n    for img_name in os.listdir(image_folder):\n        try:\n            img_path = os.path.join(image_folder, img_name)\n            image = Image.open(img_path).convert(\"RGB\")\n            image_input = clip_preprocess(image).unsqueeze(0).to(device).to(torch.float32)\n\n            images[img_name] = image_input\n        except Exception as e:\n            print(f\"Error with {img_name}: {e}\")\n    return images\n\n# tokenizer the caption for the model\ndef tokenize_caption(caption, vocab, max_len=20):\n    words = caption.lower().split()\n    \n    # include start/end tokens\n    tokens = [vocab['<start>']] + [vocab.get(w, vocab['<unk>']) for w in words] + [vocab['<end>']]\n\n    # If the sequence is longer than max_len\n    if len(tokens) > max_len:\n        tokens = tokens[:max_len]\n        tokens[-1] = vocab['<end>']\n\n    # If shorter, pad it\n    if len(tokens) < max_len:\n        tokens += [vocab['<pad>']] * (max_len - len(tokens))\n\n    return tokens\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T15:05:48.171957Z","iopub.execute_input":"2024-12-18T15:05:48.172310Z","iopub.status.idle":"2024-12-18T15:05:48.180996Z","shell.execute_reply.started":"2024-12-18T15:05:48.172278Z","shell.execute_reply":"2024-12-18T15:05:48.180119Z"},"id":"qPc-XD8g_sKD","trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ncaptions_file = \"/kaggle/input/captions.txt\"\nimage_folder = \"/kaggle/input/Images\"\n\n# Load captions\ncaptions = load_captions(captions_file)\n\n# Preprocess images\npreprocessed_images = preprocess_images(image_folder, clip.load(\"ViT-B/32\")[1], device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:05:48.615242Z","iopub.execute_input":"2024-12-18T15:05:48.615836Z","iopub.status.idle":"2024-12-18T15:07:29.417537Z","shell.execute_reply.started":"2024-12-18T15:05:48.615801Z","shell.execute_reply":"2024-12-18T15:07:29.416469Z"}},"outputs":[{"name":"stderr","text":"100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 156MiB/s]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(f\"Captions Sample: {captions[:5]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:29.419101Z","iopub.execute_input":"2024-12-18T15:07:29.419382Z","iopub.status.idle":"2024-12-18T15:07:29.424050Z","shell.execute_reply.started":"2024-12-18T15:07:29.419352Z","shell.execute_reply":"2024-12-18T15:07:29.423338Z"}},"outputs":[{"name":"stdout","text":"Captions Sample: [('1000268201_693b08cb0e.jpg', 'A child in a pink dress is climbing up a set of stairs in an entry way .'), ('1000268201_693b08cb0e.jpg', 'A girl going into a wooden building .'), ('1000268201_693b08cb0e.jpg', 'A little girl climbing into a wooden playhouse .'), ('1000268201_693b08cb0e.jpg', 'A little girl climbing the stairs to her playhouse .'), ('1000268201_693b08cb0e.jpg', 'A little girl in a pink dress going into a wooden cabin .')]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from collections import Counter\n\ndef build_vocab(captions, min_freq=1):\n    word_counts = Counter()\n\n    # Tokenize and count words\n    for _, caption in captions:\n        words = caption.lower().split()  # Simple split by spaces\n        word_counts.update(words)\n    \n    # create the vocabulary with special tokens\n    vocab = {\"<pad>\": 0, \"<unk>\": 1, \"<start>\": 2, \"<end>\": 3}\n    index = 4\n\n    for word, count in word_counts.items():\n        if count >= min_freq:\n            vocab[word] = index\n            index += 1\n\n    return vocab\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:29.425150Z","iopub.execute_input":"2024-12-18T15:07:29.425615Z","iopub.status.idle":"2024-12-18T15:07:29.442746Z","shell.execute_reply.started":"2024-12-18T15:07:29.425557Z","shell.execute_reply":"2024-12-18T15:07:29.441616Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"vocab = build_vocab(captions, min_freq=1)\nprint(f\"Vocabulary size: {len(vocab)}\")\nprint(f\"Words: {list(vocab.items())[:10]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:29.446510Z","iopub.execute_input":"2024-12-18T15:07:29.446880Z","iopub.status.idle":"2024-12-18T15:07:29.594966Z","shell.execute_reply.started":"2024-12-18T15:07:29.446825Z","shell.execute_reply":"2024-12-18T15:07:29.594146Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 9184\nWords: [('<pad>', 0), ('<unk>', 1), ('<start>', 2), ('<end>', 3), ('a', 4), ('child', 5), ('in', 6), ('pink', 7), ('dress', 8), ('is', 9)]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Define a dataset for Flickr8k ###","metadata":{}},{"cell_type":"code","source":"# For this model, we used the Flickr8k dataset and it's loaded to Kaggle's Input\n\nfrom torch.utils.data import Dataset\n\nclass FlickrDataset(Dataset):\n    def __init__(self, captions, images, vocab):\n        self.captions = [(img, cap) for img, cap in captions if img in images]\n        self.images = images\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.captions)\n\n    def __getitem__(self, idx):\n        img_name, caption = self.captions[idx]\n        if img_name not in self.images:\n            raise ValueError(f\"{img_name} not found in preprocessed images!\")\n        \n        return img_name, torch.tensor(tokenize_caption(caption, self.vocab))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"execution":{"iopub.status.busy":"2024-12-18T15:07:29.596570Z","iopub.execute_input":"2024-12-18T15:07:29.596963Z","iopub.status.idle":"2024-12-18T15:07:29.603543Z","shell.execute_reply.started":"2024-12-18T15:07:29.596925Z","shell.execute_reply":"2024-12-18T15:07:29.602951Z"},"id":"5FG_dsa-Ax2p","outputId":"2249fc7e-f270-45f7-b5ea-540496d37541","trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Define a function to split the data to training set and test  set ###","metadata":{}},{"cell_type":"code","source":"# Spilt the dataset to training set and test set\n\nfrom sklearn.model_selection import train_test_split\n\ndef split_data(captions, test_size=0.2, random_state=42):\n\n    img_names = list(set([img for img, _ in captions]))\n    train_imgs, test_imgs = train_test_split(img_names, test_size=test_size, random_state=random_state)\n\n    train_captions = [(img, cap) for img, cap in captions if img in train_imgs]\n    test_captions = [(img, cap) for img, cap in captions if img in test_imgs]\n\n    return train_captions, test_captions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:29.604255Z","iopub.execute_input":"2024-12-18T15:07:29.604516Z","iopub.status.idle":"2024-12-18T15:07:30.290421Z","shell.execute_reply.started":"2024-12-18T15:07:29.604487Z","shell.execute_reply":"2024-12-18T15:07:30.289626Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Split the captions\ntrain_captions, test_captions = split_data(captions, test_size=0.2)\n\ntrain_dataset = FlickrDataset(train_captions, preprocessed_images, vocab)\ntest_dataset = FlickrDataset(test_captions, preprocessed_images, vocab)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nprint(f\"Training samples: {len(train_dataset)}, Testing samples: {len(test_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:30.291641Z","iopub.execute_input":"2024-12-18T15:07:30.292107Z","iopub.status.idle":"2024-12-18T15:07:34.093478Z","shell.execute_reply.started":"2024-12-18T15:07:30.292069Z","shell.execute_reply":"2024-12-18T15:07:34.092667Z"}},"outputs":[{"name":"stdout","text":"Training samples: 32360, Testing samples: 8095\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Construct the CLIP Encoder and LSTM Decoder we will be using ###","metadata":{}},{"cell_type":"code","source":"# Encoder we used the pretrained CLIP model \"ViT-B/32\"\n\nclass CLIPEncoder(nn.Module):\n    def __init__(self, device):\n        super(CLIPEncoder, self).__init__()\n        self.device = device\n        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n\n    def forward(self, image):\n        with torch.no_grad():\n            features = self.model.encode_image(image)\n            features = features / features.norm(dim=-1, keepdim=True)  # Normalize\n        return features.unsqueeze(1) \n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T15:07:34.094518Z","iopub.execute_input":"2024-12-18T15:07:34.094798Z","iopub.status.idle":"2024-12-18T15:07:34.100070Z","shell.execute_reply.started":"2024-12-18T15:07:34.094773Z","shell.execute_reply":"2024-12-18T15:07:34.099111Z"},"id":"4jCfzqeiAzuv","trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Include attention mechanism for the model to make better predictions\n\nclass Attention(nn.Module):\n    def __init__(self, encoder_dim):\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, 512)\n        self.decoder_att = nn.Linear(512, 512)\n        self.full_att = nn.Linear(512, 1)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, encoder_out, decoder_hidden):\n        att1 = self.encoder_att(encoder_out)\n        att2 = self.decoder_att(decoder_hidden)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n        alpha = self.softmax(att)\n        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n        return context, alpha\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T15:07:34.101169Z","iopub.execute_input":"2024-12-18T15:07:34.101466Z","iopub.status.idle":"2024-12-18T15:07:34.111165Z","shell.execute_reply.started":"2024-12-18T15:07:34.101440Z","shell.execute_reply":"2024-12-18T15:07:34.110410Z"},"id":"hqDAjxopA3Qp","trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# For the decoder, we used the LSTM\n\nclass Decoder(nn.Module):\n    def __init__(self, vocabulary_size, encoder_dim, tf=False):\n        super(Decoder, self).__init__()\n        self.use_tf = tf\n        self.vocabulary_size = vocabulary_size\n        self.encoder_dim = encoder_dim\n\n        self.init_h = nn.Linear(encoder_dim, 512)\n        self.init_c = nn.Linear(encoder_dim, 512)\n        self.tanh = nn.Tanh()\n\n        self.f_beta = nn.Linear(512, encoder_dim)\n        self.sigmoid = nn.Sigmoid()\n\n        self.deep_output = nn.Linear(512, vocabulary_size)\n        self.dropout = nn.Dropout()\n\n        self.attention = Attention(encoder_dim)\n        self.embedding = nn.Embedding(vocabulary_size, 512)\n        self.lstm = nn.LSTMCell(512 + encoder_dim, 512)\n\n    def forward(self, img_features, captions):\n        batch_size = img_features.size(0)\n        max_timespan = captions.size(1) - 1\n\n        h, c = self.get_init_lstm_state(img_features)\n\n        embedding = self.embedding(captions)\n\n        preds = torch.zeros(batch_size, max_timespan, self.vocabulary_size).to(img_features.device)\n        alphas = torch.zeros(batch_size, max_timespan, img_features.size(1)).to(img_features.device)\n\n        for t in range(max_timespan):\n            context, alpha = self.attention(img_features, h)\n            gate = self.sigmoid(self.f_beta(h))\n            gated_context = gate * context\n\n            lstm_input = torch.cat((embedding[:, t], gated_context), dim=1)\n            h, c = self.lstm(lstm_input, (h, c))\n            preds[:, t] = self.deep_output(self.dropout(h))\n            alphas[:, t] = alpha\n\n        return preds, alphas\n\n    def get_init_lstm_state(self, img_features):\n        img_features = img_features.to(torch.float32)\n        avg_features = img_features.mean(dim=1)\n        c = self.tanh(self.init_c(avg_features))\n        h = self.tanh(self.init_h(avg_features))\n        return h, c\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T15:07:34.114117Z","iopub.execute_input":"2024-12-18T15:07:34.114339Z","iopub.status.idle":"2024-12-18T15:07:34.123357Z","shell.execute_reply.started":"2024-12-18T15:07:34.114318Z","shell.execute_reply":"2024-12-18T15:07:34.122651Z"},"id":"R1GgPx3-A464","trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# construct the encoder decoder structure\n\nclass EncoderDecoder(nn.Module):\n    def __init__(self, vocab_size, device):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = CLIPEncoder(device)\n        self.decoder = Decoder(vocab_size, encoder_dim=512)\n\n    def forward(self, images, captions):\n        encoder_out = self.encoder(images)\n        preds, alphas = self.decoder(encoder_out, captions)\n        return preds, alphas\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T15:07:34.124188Z","iopub.execute_input":"2024-12-18T15:07:34.124398Z","iopub.status.idle":"2024-12-18T15:07:34.138166Z","shell.execute_reply.started":"2024-12-18T15:07:34.124376Z","shell.execute_reply":"2024-12-18T15:07:34.137223Z"},"id":"rRWySoCgA7OA","trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Training the Encoder Decoder model ###","metadata":{}},{"cell_type":"code","source":"losses = []\ndef train_model(model, data_loader, criterion, optimizer, vocab_size, device, num_epochs=20):\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        for img_names, captions in data_loader:\n            valid_images = []\n            for name in img_names:\n                if name in preprocessed_images:\n                    valid_images.append(preprocessed_images[name])\n                else:\n                    print(f\"Skipping invalid key: {name}\")\n\n            if not valid_images:\n                continue  # Skip batch if no valid images\n\n            # Combine valid images into a tensor\n            images = torch.cat(valid_images).to(device)\n            captions = captions.to(device)\n\n            # Forward pass\n            preds, alphas = model(images, captions)\n\n            max_seq_len = captions.size(1) - 1  # Exclude <end> token\n            preds = preds[:, :max_seq_len, :].contiguous()\n            targets = captions[:, 1:max_seq_len + 1].contiguous()\n\n            loss = criterion(preds.view(-1, vocab_size), targets.view(-1))\n\n            # Backpropagation\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            \n        losses.append(total_loss/len(data_loader))\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(data_loader)}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T15:07:34.139457Z","iopub.execute_input":"2024-12-18T15:07:34.139755Z","iopub.status.idle":"2024-12-18T15:07:34.155264Z","shell.execute_reply.started":"2024-12-18T15:07:34.139731Z","shell.execute_reply":"2024-12-18T15:07:34.154517Z"},"id":"-C6q_Sh_A9Nr","trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"model = EncoderDecoder(len(vocab), device).to(device)\nmodel = model.to(torch.float32)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.Adam(model.parameters(), lr=2e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:34.156354Z","iopub.execute_input":"2024-12-18T15:07:34.157001Z","iopub.status.idle":"2024-12-18T15:07:38.689600Z","shell.execute_reply.started":"2024-12-18T15:07:34.156962Z","shell.execute_reply":"2024-12-18T15:07:38.688902Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"print(f\"Vocabulary size passed to Decoder: {len(vocab)}\")\nprint(f\"Decoder's vocabulary size: {model.decoder.vocabulary_size}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:38.690616Z","iopub.execute_input":"2024-12-18T15:07:38.690882Z","iopub.status.idle":"2024-12-18T15:07:38.695513Z","shell.execute_reply.started":"2024-12-18T15:07:38.690857Z","shell.execute_reply":"2024-12-18T15:07:38.694615Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size passed to Decoder: 9184\nDecoder's vocabulary size: 9184\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"train_model(model, train_loader, criterion, optimizer, len(vocab), device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:38.696531Z","iopub.execute_input":"2024-12-18T15:07:38.696838Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20, Loss: 4.2563004993167315\nEpoch 2/20, Loss: 3.4385771393304756\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\nepochs = list(range(1, 11))  # Epochs from 1 to 10\n# losses = [\n#     4.6180592459652265, 3.7734965309795183, 3.4776065856100544,\n#     3.278049479163852, 3.1304872481248123, 3.012644949166671,\n#     2.914686327395232, 2.8316029385615713, 2.7572919848879334,\n#     2.69440748050575\n# ]\n\n# Plot the training loss curve\nplt.figure(figsize=(8, 6))\nplt.plot(epochs, losses, marker='o', linestyle='-', color='b')\nplt.title(\"Training Loss Curve\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.grid()\nplt.show() \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T04:48:46.659466Z","iopub.execute_input":"2024-12-18T04:48:46.660137Z","iopub.status.idle":"2024-12-18T04:48:46.920932Z","shell.execute_reply.started":"2024-12-18T04:48:46.660104Z","shell.execute_reply":"2024-12-18T04:48:46.920124Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# Save the model\ntorch.save(model, \"trained_model_full_2e^-4.pth\")\nprint(\"Model saved\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T04:53:47.923232Z","iopub.execute_input":"2024-12-18T04:53:47.924151Z","iopub.status.idle":"2024-12-18T04:53:48.805249Z","shell.execute_reply.started":"2024-12-18T04:53:47.924114Z","shell.execute_reply":"2024-12-18T04:53:48.804346Z"}},"outputs":[{"name":"stdout","text":"Model saved\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Load the model\nmodel = torch.load(\"trained_model_full.pth\")\nmodel.to(device)\nmodel.eval()\nprint(\"Model loaded\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T02:58:32.136459Z","iopub.execute_input":"2024-12-18T02:58:32.137062Z","iopub.status.idle":"2024-12-18T02:58:32.621651Z","shell.execute_reply.started":"2024-12-18T02:58:32.137027Z","shell.execute_reply":"2024-12-18T02:58:32.620725Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/53289200.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(\"trained_model_full.pth\")\n","output_type":"stream"},{"name":"stdout","text":"Model loaded\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def beam_search_caption(model, image, vocab, device, beam_size=3, max_len=20):\n\n    model.eval()\n    idx_to_word = {v: k for k, v in vocab.items()}  # Reverse vocab for decoding\n\n    # Encode the image using the encoder\n    with torch.no_grad():\n        encoder_out = model.encoder(image) \n\n    # Initialize the LSTM hidden states\n    h, c = model.decoder.get_init_lstm_state(encoder_out)\n\n    start_token = vocab['<start>']\n    end_token = vocab['<end>']\n\n    # Beam search initialization\n    sequences = [(0.0, [start_token], h, c)]  # (cumulative log probability, sequence, hidden state, cell state)\n\n    # Iterate over each timestep\n    for _ in range(max_len):\n        all_candidates = []\n\n        for score, seq, h, c in sequences:\n            # If the last token is <end>, keep the sequence as is\n            if seq[-1] == end_token:\n                all_candidates.append((score, seq, h, c))\n                continue\n\n            # Get the last word and embed it\n            prev_word = torch.tensor([seq[-1]]).to(device)\n            embedding = model.decoder.embedding(prev_word)\n\n            # Compute attention and LSTM outputs\n            context, _ = model.decoder.attention(encoder_out, h)\n            gate = model.decoder.sigmoid(model.decoder.f_beta(h))\n            gated_context = gate * context\n\n            lstm_input = torch.cat((embedding, gated_context), dim=1)\n            h, c = model.decoder.lstm(lstm_input, (h, c))\n\n            # Predict the next word\n            logits = model.decoder.deep_output(h)  # (1, vocab_size)\n            probs = torch.log_softmax(logits, dim=1)  # Convert logits to log probabilities\n\n            # Get the top beam_size words and their log probabilities\n            top_probs, top_indices = probs.topk(beam_size)\n\n            # Create new candidates\n            for i in range(beam_size):\n                new_score = score + top_probs[0, i].item()  # Update cumulative score\n                new_seq = seq + [top_indices[0, i].item()]\n                all_candidates.append((new_score, new_seq, h, c))\n\n        # Sort all candidates by score and select the top beam_size sequences\n        ordered = sorted(all_candidates, key=lambda x: x[0], reverse=True)\n        sequences = ordered[:beam_size]\n\n    # Choose the best sequence (highest cumulative score)\n    best_score, best_seq, _, _ = sequences[0]\n    best_caption = [idx_to_word[idx] for idx in best_seq if idx not in [start_token, end_token, vocab['<pad>']]]\n\n    return best_caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T03:30:39.640012Z","iopub.execute_input":"2024-12-18T03:30:39.640698Z","iopub.status.idle":"2024-12-18T03:30:39.651056Z","shell.execute_reply.started":"2024-12-18T03:30:39.640665Z","shell.execute_reply":"2024-12-18T03:30:39.650150Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def generate_and_save_captions_with_beam_search(model, test_loader, vocab, device, beam_size=3, max_len=20, output_file=\"captions_output.txt\"):\n\n    model.eval()\n    idx_to_word = {v: k for k, v in vocab.items()}  # Reverse vocab for decoding\n    results = []  # To store image name, actual caption, and generated caption\n\n    with torch.no_grad():\n        for img_names, captions in test_loader:\n            for name, actual_tokens in zip(img_names, captions):\n                # Retrieve preprocessed image tensor\n                if name in preprocessed_images:\n                    image = preprocessed_images[name].to(device)\n                else:\n                    print(f\"Warning: Image {name} not found in preprocessed images.\")\n                    continue\n\n                # Generate caption using beam search\n                generated_caption = beam_search_caption(\n                    model, image, vocab, device, beam_size, max_len\n                )\n\n                # Convert actual tokens to words (filter out padding and special tokens)\n                actual_caption = [\n                    idx_to_word[idx.item()]\n                    for idx in actual_tokens\n                    if idx.item() not in [vocab['<pad>'], vocab['<start>'], vocab['<end>']]\n                ]\n\n                # Save results\n                results.append({\n                    \"image_name\": name,\n                    \"actual_caption\": \" \".join(actual_caption),\n                    \"generated_caption\": \" \".join(generated_caption),\n                })\n\n    # Save all results to a file\n    with open(output_file, \"w\") as f:\n        for result in results:\n            f.write(\n                f\"Image: {result['image_name']}\\n\"\n                f\"Actual Caption: {result['actual_caption']}\\n\"\n                f\"Generated Caption: {result['generated_caption']}\\n\"\n                \"--------\\n\"\n            )\n\n    # Display the first 10 results\n    for result in results[:10]:\n        print(f\"Image: {result['image_name']}\")\n        print(f\"Actual Caption: {result['actual_caption']}\")\n        print(f\"Generated Caption: {result['generated_caption']}\")\n        print(\"--------\")\n\n    return results  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T03:30:40.976016Z","iopub.execute_input":"2024-12-18T03:30:40.976658Z","iopub.status.idle":"2024-12-18T03:30:40.984905Z","shell.execute_reply.started":"2024-12-18T03:30:40.976626Z","shell.execute_reply":"2024-12-18T03:30:40.983944Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"results = generate_and_save_captions_with_beam_search(\n    model, test_loader, vocab, device, beam_size=10, max_len=20, output_file=\"captions_output.txt\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T03:47:59.499644Z","iopub.execute_input":"2024-12-18T03:47:59.500009Z","iopub.status.idle":"2024-12-18T03:58:55.016412Z","shell.execute_reply.started":"2024-12-18T03:47:59.499978Z","shell.execute_reply":"2024-12-18T03:58:55.014573Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_and_save_captions_with_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcaptions_output.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[26], line 35\u001b[0m, in \u001b[0;36mgenerate_and_save_captions_with_beam_search\u001b[0;34m(model, test_loader, vocab, device, beam_size, max_len, output_file)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Generate caption using beam search\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m generated_caption \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_search_caption\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Convert actual tokens to words (filter out padding and special tokens)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m actual_caption \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     41\u001b[0m     idx_to_word[idx\u001b[38;5;241m.\u001b[39mitem()]\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m actual_tokens\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m], vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<start>\u001b[39m\u001b[38;5;124m'\u001b[39m], vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<end>\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     44\u001b[0m ]\n","Cell \u001b[0;32mIn[25], line 45\u001b[0m, in \u001b[0;36mbeam_search_caption\u001b[0;34m(model, image, vocab, device, beam_size, max_len)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Get the last word and embed it\u001b[39;00m\n\u001b[1;32m     44\u001b[0m prev_word \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([seq[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 45\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_word\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Compute attention and LSTM outputs\u001b[39;00m\n\u001b[1;32m     48\u001b[0m context, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mattention(encoder_out, h)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":37},{"cell_type":"code","source":"actual_captions = [result['actual_caption'] for result in results]\ngenerated_captions = [result['generated_caption'] for result in results]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T03:42:30.337650Z","iopub.execute_input":"2024-12-18T03:42:30.338026Z","iopub.status.idle":"2024-12-18T03:42:30.346020Z","shell.execute_reply.started":"2024-12-18T03:42:30.337993Z","shell.execute_reply":"2024-12-18T03:42:30.344926Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"!pip install nltk\nimport nltk\nnltk.download('punkt')  # If needed\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\nsmooth = SmoothingFunction().method1\n\ndef clean_caption(caption_tokens):\n    return [w for w in caption_tokens if w not in ['<start>', '<end>', '<pad>']]\n\n# Clean up captions before BLEU calculation\nclean_actual = [clean_caption(ref) for ref in actual_captions]\nclean_generated = [clean_caption(hyp) for hyp in generated_captions]\n\nall_scores = []\nfor ref, hyp in zip(clean_actual, clean_generated):\n    reference = [ref]\n    hypothesis = hyp\n    score = sentence_bleu(reference, hypothesis, smoothing_function=smooth)\n    all_scores.append(score)\n\naverage_bleu = sum(all_scores) / len(all_scores)\nprint(f\"Average BLEU score (cleaned): {average_bleu}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T03:44:41.704857Z","iopub.execute_input":"2024-12-18T03:44:41.705259Z","iopub.status.idle":"2024-12-18T03:44:51.640671Z","shell.execute_reply.started":"2024-12-18T03:44:41.705228Z","shell.execute_reply":"2024-12-18T03:44:51.639532Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nAverage BLEU score (cleaned): 0.2779620892391385\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}