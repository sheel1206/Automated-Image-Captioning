{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom PIL import Image\n\n!pip install git+https://github.com/openai/CLIP.git\n\nimport clip\nimport numpy as np","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-12-18T15:05:26.717271Z","iopub.execute_input":"2024-12-18T15:05:26.717520Z","iopub.status.idle":"2024-12-18T15:05:45.536385Z","shell.execute_reply.started":"2024-12-18T15:05:26.717494Z","shell.execute_reply":"2024-12-18T15:05:45.535445Z"},"id":"1Gc-D0P0_kn1","outputId":"19622d27-42a7-4fd5-c74f-926f2b8a04ee","trusted":true},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ylk5qpru\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ylk5qpru\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2024.5.15)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.19.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->clip==1.0) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2024.6.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=d2306e6d3b967c7d782fb22dca9013de2b762a107b2031eadc929ed8f133d791\n  Stored in directory: /tmp/pip-ephem-wheel-cache-6ltk5ctw/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.3.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Utility Functions that load captions, preprocess images and tokenize captions ###","metadata":{}},{"cell_type":"code","source":"# Some utility functions are defined above\n\n# Load captions\ndef load_captions(captions_file):\n    captions = []\n    with open(captions_file, 'r') as f:\n        for line in f:\n            img, caption = line.strip().split(',', 1)  # Split on the first comma\n            if (img != 'image'):\n                captions.append((img.strip(), caption.strip()))\n    return captions\n\n\n# Preprocess images using CLIP\ndef preprocess_images(image_folder, clip_preprocess, device):\n    images = {}\n    for img_name in os.listdir(image_folder):\n        try:\n            img_path = os.path.join(image_folder, img_name)\n            image = Image.open(img_path).convert(\"RGB\")\n            image_input = clip_preprocess(image).unsqueeze(0).to(device).to(torch.float32)\n\n            images[img_name] = image_input\n        except Exception as e:\n            print(f\"Error with {img_name}: {e}\")\n    return images\n\n# tokenizer the caption for the model\ndef tokenize_caption(caption, vocab, max_len=20):\n    words = caption.lower().split()\n    \n    # include start/end tokens\n    tokens = [vocab['<start>']] + [vocab.get(w, vocab['<unk>']) for w in words] + [vocab['<end>']]\n\n    # If the sequence is longer than max_len\n    if len(tokens) > max_len:\n        tokens = tokens[:max_len]\n        tokens[-1] = vocab['<end>']\n\n    # If shorter, pad it\n    if len(tokens) < max_len:\n        tokens += [vocab['<pad>']] * (max_len - len(tokens))\n\n    return tokens\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T15:05:48.171957Z","iopub.execute_input":"2024-12-18T15:05:48.172310Z","iopub.status.idle":"2024-12-18T15:05:48.180996Z","shell.execute_reply.started":"2024-12-18T15:05:48.172278Z","shell.execute_reply":"2024-12-18T15:05:48.180119Z"},"id":"qPc-XD8g_sKD","trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ncaptions_file = \"/kaggle/input/captions.txt\"\nimage_folder = \"/kaggle/input/Images\"\n\n# Load captions\ncaptions = load_captions(captions_file)\n\n# Preprocess images\npreprocessed_images = preprocess_images(image_folder, clip.load(\"ViT-B/32\")[1], device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:05:48.615242Z","iopub.execute_input":"2024-12-18T15:05:48.615836Z","iopub.status.idle":"2024-12-18T15:07:29.417537Z","shell.execute_reply.started":"2024-12-18T15:05:48.615801Z","shell.execute_reply":"2024-12-18T15:07:29.416469Z"}},"outputs":[{"name":"stderr","text":"100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 156MiB/s]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(f\"Captions Sample: {captions[:5]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:29.419101Z","iopub.execute_input":"2024-12-18T15:07:29.419382Z","iopub.status.idle":"2024-12-18T15:07:29.424050Z","shell.execute_reply.started":"2024-12-18T15:07:29.419352Z","shell.execute_reply":"2024-12-18T15:07:29.423338Z"}},"outputs":[{"name":"stdout","text":"Captions Sample: [('1000268201_693b08cb0e.jpg', 'A child in a pink dress is climbing up a set of stairs in an entry way .'), ('1000268201_693b08cb0e.jpg', 'A girl going into a wooden building .'), ('1000268201_693b08cb0e.jpg', 'A little girl climbing into a wooden playhouse .'), ('1000268201_693b08cb0e.jpg', 'A little girl climbing the stairs to her playhouse .'), ('1000268201_693b08cb0e.jpg', 'A little girl in a pink dress going into a wooden cabin .')]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from collections import Counter\n\ndef build_vocab(captions, min_freq=1):\n    word_counts = Counter()\n\n    # Tokenize and count words\n    for _, caption in captions:\n        words = caption.lower().split()  # Simple split by spaces\n        word_counts.update(words)\n    \n    # create the vocabulary with special tokens\n    vocab = {\"<pad>\": 0, \"<unk>\": 1, \"<start>\": 2, \"<end>\": 3}\n    index = 4\n\n    for word, count in word_counts.items():\n        if count >= min_freq:\n            vocab[word] = index\n            index += 1\n\n    return vocab\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:29.425150Z","iopub.execute_input":"2024-12-18T15:07:29.425615Z","iopub.status.idle":"2024-12-18T15:07:29.442746Z","shell.execute_reply.started":"2024-12-18T15:07:29.425557Z","shell.execute_reply":"2024-12-18T15:07:29.441616Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"vocab = build_vocab(captions, min_freq=1)\nprint(f\"Vocabulary size: {len(vocab)}\")\nprint(f\"Words: {list(vocab.items())[:10]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:29.446510Z","iopub.execute_input":"2024-12-18T15:07:29.446880Z","iopub.status.idle":"2024-12-18T15:07:29.594966Z","shell.execute_reply.started":"2024-12-18T15:07:29.446825Z","shell.execute_reply":"2024-12-18T15:07:29.594146Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 9184\nWords: [('<pad>', 0), ('<unk>', 1), ('<start>', 2), ('<end>', 3), ('a', 4), ('child', 5), ('in', 6), ('pink', 7), ('dress', 8), ('is', 9)]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Define a dataset for Flickr8k ###","metadata":{}},{"cell_type":"code","source":"# For this model, we used the Flickr8k dataset and it's loaded to Kaggle's Input\n\nfrom torch.utils.data import Dataset\n\nclass FlickrDataset(Dataset):\n    def __init__(self, captions, images, vocab):\n        self.captions = [(img, cap) for img, cap in captions if img in images]\n        self.images = images\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.captions)\n\n    def __getitem__(self, idx):\n        img_name, caption = self.captions[idx]\n        if img_name not in self.images:\n            raise ValueError(f\"{img_name} not found in preprocessed images!\")\n        \n        return img_name, torch.tensor(tokenize_caption(caption, self.vocab))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"execution":{"iopub.status.busy":"2024-12-18T15:07:29.596570Z","iopub.execute_input":"2024-12-18T15:07:29.596963Z","iopub.status.idle":"2024-12-18T15:07:29.603543Z","shell.execute_reply.started":"2024-12-18T15:07:29.596925Z","shell.execute_reply":"2024-12-18T15:07:29.602951Z"},"id":"5FG_dsa-Ax2p","outputId":"2249fc7e-f270-45f7-b5ea-540496d37541","trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Define a function to split the data to training set and test  set ###","metadata":{}},{"cell_type":"code","source":"# Spilt the dataset to training set and test set\n\nfrom sklearn.model_selection import train_test_split\n\ndef split_data(captions, test_size=0.2, random_state=42):\n\n    img_names = list(set([img for img, _ in captions]))\n    train_imgs, test_imgs = train_test_split(img_names, test_size=test_size, random_state=random_state)\n\n    train_captions = [(img, cap) for img, cap in captions if img in train_imgs]\n    test_captions = [(img, cap) for img, cap in captions if img in test_imgs]\n\n    return train_captions, test_captions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:29.604255Z","iopub.execute_input":"2024-12-18T15:07:29.604516Z","iopub.status.idle":"2024-12-18T15:07:30.290421Z","shell.execute_reply.started":"2024-12-18T15:07:29.604487Z","shell.execute_reply":"2024-12-18T15:07:30.289626Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Split the captions\ntrain_captions, test_captions = split_data(captions, test_size=0.2)\n\ntrain_dataset = FlickrDataset(train_captions, preprocessed_images, vocab)\ntest_dataset = FlickrDataset(test_captions, preprocessed_images, vocab)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nprint(f\"Training samples: {len(train_dataset)}, Testing samples: {len(test_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:30.291641Z","iopub.execute_input":"2024-12-18T15:07:30.292107Z","iopub.status.idle":"2024-12-18T15:07:34.093478Z","shell.execute_reply.started":"2024-12-18T15:07:30.292069Z","shell.execute_reply":"2024-12-18T15:07:34.092667Z"}},"outputs":[{"name":"stdout","text":"Training samples: 32360, Testing samples: 8095\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Construct the CLIP Encoder and LSTM Decoder we will be using ###","metadata":{}},{"cell_type":"code","source":"# Encoder we used the pretrained CLIP model \"ViT-B/32\"\n\nclass CLIPEncoder(nn.Module):\n    def __init__(self, device):\n        super(CLIPEncoder, self).__init__()\n        self.device = device\n        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n\n    def forward(self, image):\n        with torch.no_grad():\n            features = self.model.encode_image(image)\n            features = features / features.norm(dim=-1, keepdim=True)  # Normalize\n        return features.unsqueeze(1) \n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T15:07:34.094518Z","iopub.execute_input":"2024-12-18T15:07:34.094798Z","iopub.status.idle":"2024-12-18T15:07:34.100070Z","shell.execute_reply.started":"2024-12-18T15:07:34.094773Z","shell.execute_reply":"2024-12-18T15:07:34.099111Z"},"id":"4jCfzqeiAzuv","trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Include attention mechanism for the model to make better predictions\n\nclass Attention(nn.Module):\n    def __init__(self, encoder_dim):\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, 512)\n        self.decoder_att = nn.Linear(512, 512)\n        self.full_att = nn.Linear(512, 1)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, encoder_out, decoder_hidden):\n        att1 = self.encoder_att(encoder_out)\n        att2 = self.decoder_att(decoder_hidden)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n        alpha = self.softmax(att)\n        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n        return context, alpha\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T15:07:34.101169Z","iopub.execute_input":"2024-12-18T15:07:34.101466Z","iopub.status.idle":"2024-12-18T15:07:34.111165Z","shell.execute_reply.started":"2024-12-18T15:07:34.101440Z","shell.execute_reply":"2024-12-18T15:07:34.110410Z"},"id":"hqDAjxopA3Qp","trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# For the decoder, we used the LSTM\n\nclass Decoder(nn.Module):\n    def __init__(self, vocabulary_size, encoder_dim, tf=False):\n        super(Decoder, self).__init__()\n        self.use_tf = tf\n        self.vocabulary_size = vocabulary_size\n        self.encoder_dim = encoder_dim\n\n        self.init_h = nn.Linear(encoder_dim, 512)\n        self.init_c = nn.Linear(encoder_dim, 512)\n        self.tanh = nn.Tanh()\n\n        self.f_beta = nn.Linear(512, encoder_dim)\n        self.sigmoid = nn.Sigmoid()\n\n        self.deep_output = nn.Linear(512, vocabulary_size)\n        self.dropout = nn.Dropout()\n\n        self.attention = Attention(encoder_dim)\n        self.embedding = nn.Embedding(vocabulary_size, 512)\n        self.lstm = nn.LSTMCell(512 + encoder_dim, 512)\n\n    def forward(self, img_features, captions):\n        batch_size = img_features.size(0)\n        max_timespan = captions.size(1) - 1\n\n        h, c = self.get_init_lstm_state(img_features)\n\n        embedding = self.embedding(captions)\n\n        preds = torch.zeros(batch_size, max_timespan, self.vocabulary_size).to(img_features.device)\n        alphas = torch.zeros(batch_size, max_timespan, img_features.size(1)).to(img_features.device)\n\n        for t in range(max_timespan):\n            context, alpha = self.attention(img_features, h)\n            gate = self.sigmoid(self.f_beta(h))\n            gated_context = gate * context\n\n            lstm_input = torch.cat((embedding[:, t], gated_context), dim=1)\n            h, c = self.lstm(lstm_input, (h, c))\n            preds[:, t] = self.deep_output(self.dropout(h))\n            alphas[:, t] = alpha\n\n        return preds, alphas\n\n    def get_init_lstm_state(self, img_features):\n        img_features = img_features.to(torch.float32)\n        avg_features = img_features.mean(dim=1)\n        c = self.tanh(self.init_c(avg_features))\n        h = self.tanh(self.init_h(avg_features))\n        return h, c\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T15:07:34.114117Z","iopub.execute_input":"2024-12-18T15:07:34.114339Z","iopub.status.idle":"2024-12-18T15:07:34.123357Z","shell.execute_reply.started":"2024-12-18T15:07:34.114318Z","shell.execute_reply":"2024-12-18T15:07:34.122651Z"},"id":"R1GgPx3-A464","trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# construct the encoder decoder structure\n\nclass EncoderDecoder(nn.Module):\n    def __init__(self, vocab_size, device):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = CLIPEncoder(device)\n        self.decoder = Decoder(vocab_size, encoder_dim=512)\n\n    def forward(self, images, captions):\n        encoder_out = self.encoder(images)\n        preds, alphas = self.decoder(encoder_out, captions)\n        return preds, alphas\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T15:07:34.124188Z","iopub.execute_input":"2024-12-18T15:07:34.124398Z","iopub.status.idle":"2024-12-18T15:07:34.138166Z","shell.execute_reply.started":"2024-12-18T15:07:34.124376Z","shell.execute_reply":"2024-12-18T15:07:34.137223Z"},"id":"rRWySoCgA7OA","trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Training the Encoder Decoder model ###","metadata":{}},{"cell_type":"code","source":"losses = []\ndef train_model(model, data_loader, criterion, optimizer, vocab_size, device, num_epochs=20):\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        for img_names, captions in data_loader:\n            valid_images = []\n            for name in img_names:\n                if name in preprocessed_images:\n                    valid_images.append(preprocessed_images[name])\n                else:\n                    print(f\"Skipping invalid key: {name}\")\n\n            if not valid_images:\n                continue  # Skip batch if no valid images\n\n            # Combine valid images into a tensor\n            images = torch.cat(valid_images).to(device)\n            captions = captions.to(device)\n\n            # Forward pass\n            preds, alphas = model(images, captions)\n\n            max_seq_len = captions.size(1) - 1  # Exclude <end> token\n            preds = preds[:, :max_seq_len, :].contiguous()\n            targets = captions[:, 1:max_seq_len + 1].contiguous()\n\n            loss = criterion(preds.view(-1, vocab_size), targets.view(-1))\n\n            # Backpropagation\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            \n        losses.append(total_loss/len(data_loader))\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(data_loader)}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T15:07:34.139457Z","iopub.execute_input":"2024-12-18T15:07:34.139755Z","iopub.status.idle":"2024-12-18T15:07:34.155264Z","shell.execute_reply.started":"2024-12-18T15:07:34.139731Z","shell.execute_reply":"2024-12-18T15:07:34.154517Z"},"id":"-C6q_Sh_A9Nr","trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"model = EncoderDecoder(len(vocab), device).to(device)\nmodel = model.to(torch.float32)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.Adam(model.parameters(), lr=2e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:34.156354Z","iopub.execute_input":"2024-12-18T15:07:34.157001Z","iopub.status.idle":"2024-12-18T15:07:38.689600Z","shell.execute_reply.started":"2024-12-18T15:07:34.156962Z","shell.execute_reply":"2024-12-18T15:07:38.688902Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"print(f\"Vocabulary size passed to Decoder: {len(vocab)}\")\nprint(f\"Decoder's vocabulary size: {model.decoder.vocabulary_size}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:38.690616Z","iopub.execute_input":"2024-12-18T15:07:38.690882Z","iopub.status.idle":"2024-12-18T15:07:38.695513Z","shell.execute_reply.started":"2024-12-18T15:07:38.690857Z","shell.execute_reply":"2024-12-18T15:07:38.694615Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size passed to Decoder: 9184\nDecoder's vocabulary size: 9184\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"train_model(model, train_loader, criterion, optimizer, len(vocab), device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:07:38.696531Z","iopub.execute_input":"2024-12-18T15:07:38.696838Z","iopub.status.idle":"2024-12-18T15:56:48.926463Z","shell.execute_reply.started":"2024-12-18T15:07:38.696803Z","shell.execute_reply":"2024-12-18T15:56:48.925463Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20, Loss: 4.2563004993167315\nEpoch 2/20, Loss: 3.4385771393304756\nEpoch 3/20, Loss: 3.1385629200652656\nEpoch 4/20, Loss: 2.9441284252249678\nEpoch 5/20, Loss: 2.798993114897385\nEpoch 6/20, Loss: 2.6854407504613222\nEpoch 7/20, Loss: 2.5865051494285525\nEpoch 8/20, Loss: 2.501452310872172\nEpoch 9/20, Loss: 2.4224070050264066\nEpoch 10/20, Loss: 2.353682205493271\nEpoch 11/20, Loss: 2.2879094321265994\nEpoch 12/20, Loss: 2.228486089013782\nEpoch 13/20, Loss: 2.1714313894863655\nEpoch 14/20, Loss: 2.11873631974454\nEpoch 15/20, Loss: 2.0670199694605214\nEpoch 16/20, Loss: 2.0216209191107466\nEpoch 17/20, Loss: 1.9755747315911907\nEpoch 18/20, Loss: 1.9305142863937046\nEpoch 19/20, Loss: 1.8900666594976494\nEpoch 20/20, Loss: 1.8499988316311666\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\nepochs = list(range(1, 21))  # Epochs from 1 to 10\n# losses = [\n#     4.6180592459652265, 3.7734965309795183, 3.4776065856100544,\n#     3.278049479163852, 3.1304872481248123, 3.012644949166671,\n#     2.914686327395232, 2.8316029385615713, 2.7572919848879334,\n#     2.69440748050575\n# ]\n\n# Plot the training loss curve\nplt.figure(figsize=(8, 6))\nplt.plot(epochs, losses, marker='o', linestyle='-', color='b')\nplt.title(\"Training Loss Curve\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.grid()\nplt.show() \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:57:05.120724Z","iopub.execute_input":"2024-12-18T15:57:05.121725Z","iopub.status.idle":"2024-12-18T15:57:05.366501Z","shell.execute_reply.started":"2024-12-18T15:57:05.121676Z","shell.execute_reply":"2024-12-18T15:57:05.365643Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Save the model\ntorch.save(model, \"trained_model_full_2e^-4_20.pth\")\nprint(\"Model saved\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:57:19.444163Z","iopub.execute_input":"2024-12-18T15:57:19.444869Z","iopub.status.idle":"2024-12-18T15:57:20.322317Z","shell.execute_reply.started":"2024-12-18T15:57:19.444834Z","shell.execute_reply":"2024-12-18T15:57:20.321337Z"}},"outputs":[{"name":"stdout","text":"Model saved\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Load the model\nmodel = torch.load(\"trained_model_full.pth\")\nmodel.to(device)\nmodel.eval()\nprint(\"Model loaded\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T02:58:32.136459Z","iopub.execute_input":"2024-12-18T02:58:32.137062Z","iopub.status.idle":"2024-12-18T02:58:32.621651Z","shell.execute_reply.started":"2024-12-18T02:58:32.137027Z","shell.execute_reply":"2024-12-18T02:58:32.620725Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/53289200.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(\"trained_model_full.pth\")\n","output_type":"stream"},{"name":"stdout","text":"Model loaded\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def beam_search_caption(model, image, vocab, device, beam_size=3, max_len=20):\n\n    model.eval()\n    idx_to_word = {v: k for k, v in vocab.items()}  # Reverse vocab for decoding\n\n    # Encode the image using the encoder\n    with torch.no_grad():\n        encoder_out = model.encoder(image) \n\n    # Initialize the LSTM hidden states\n    h, c = model.decoder.get_init_lstm_state(encoder_out)\n\n    start_token = vocab['<start>']\n    end_token = vocab['<end>']\n\n    # Beam search initialization\n    sequences = [(0.0, [start_token], h, c)]  # (cumulative log probability, sequence, hidden state, cell state)\n\n    # Iterate over each timestep\n    for _ in range(max_len):\n        all_candidates = []\n\n        for score, seq, h, c in sequences:\n            # If the last token is <end>, keep the sequence as is\n            if seq[-1] == end_token:\n                all_candidates.append((score, seq, h, c))\n                continue\n\n            # Get the last word and embed it\n            prev_word = torch.tensor([seq[-1]]).to(device)\n            embedding = model.decoder.embedding(prev_word)\n\n            # Compute attention and LSTM outputs\n            context, _ = model.decoder.attention(encoder_out, h)\n            gate = model.decoder.sigmoid(model.decoder.f_beta(h))\n            gated_context = gate * context\n\n            lstm_input = torch.cat((embedding, gated_context), dim=1)\n            h, c = model.decoder.lstm(lstm_input, (h, c))\n\n            # Predict the next word\n            logits = model.decoder.deep_output(h)  # (1, vocab_size)\n            probs = torch.log_softmax(logits, dim=1)  # Convert logits to log probabilities\n\n            # Get the top beam_size words and their log probabilities\n            top_probs, top_indices = probs.topk(beam_size)\n\n            # Create new candidates\n            for i in range(beam_size):\n                new_score = score + top_probs[0, i].item()  # Update cumulative score\n                new_seq = seq + [top_indices[0, i].item()]\n                all_candidates.append((new_score, new_seq, h, c))\n\n        # Sort all candidates by score and select the top beam_size sequences\n        ordered = sorted(all_candidates, key=lambda x: x[0], reverse=True)\n        sequences = ordered[:beam_size]\n\n    # Choose the best sequence (highest cumulative score)\n    best_score, best_seq, _, _ = sequences[0]\n    best_caption = [idx_to_word[idx] for idx in best_seq if idx not in [start_token, end_token, vocab['<pad>']]]\n\n    return best_caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:57:26.597179Z","iopub.execute_input":"2024-12-18T15:57:26.597842Z","iopub.status.idle":"2024-12-18T15:57:26.607343Z","shell.execute_reply.started":"2024-12-18T15:57:26.597806Z","shell.execute_reply":"2024-12-18T15:57:26.606432Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def generate_and_save_captions_with_beam_search(model, test_loader, vocab, device, beam_size=3, max_len=20, output_file=\"captions_output.txt\"):\n\n    model.eval()\n    idx_to_word = {v: k for k, v in vocab.items()}  # Reverse vocab for decoding\n    results = []  # To store image name, actual caption, and generated caption\n\n    with torch.no_grad():\n        for img_names, captions in test_loader:\n            for name, actual_tokens in zip(img_names, captions):\n                # Retrieve preprocessed image tensor\n                if name in preprocessed_images:\n                    image = preprocessed_images[name].to(device)\n                else:\n                    print(f\"Warning: Image {name} not found in preprocessed images.\")\n                    continue\n\n                # Generate caption using beam search\n                generated_caption = beam_search_caption(\n                    model, image, vocab, device, beam_size, max_len\n                )\n\n                # Convert actual tokens to words (filter out padding and special tokens)\n                actual_caption = [\n                    idx_to_word[idx.item()]\n                    for idx in actual_tokens\n                    if idx.item() not in [vocab['<pad>'], vocab['<start>'], vocab['<end>']]\n                ]\n\n                # Save results\n                results.append({\n                    \"image_name\": name,\n                    \"actual_caption\": \" \".join(actual_caption),\n                    \"generated_caption\": \" \".join(generated_caption),\n                })\n\n    # Save all results to a file\n    with open(output_file, \"w\") as f:\n        for result in results:\n            f.write(\n                f\"Image: {result['image_name']}\\n\"\n                f\"Actual Caption: {result['actual_caption']}\\n\"\n                f\"Generated Caption: {result['generated_caption']}\\n\"\n                \"--------\\n\"\n            )\n\n    # Display the first 10 results\n    for result in results[:10]:\n        print(f\"Image: {result['image_name']}\")\n        print(f\"Actual Caption: {result['actual_caption']}\")\n        print(f\"Generated Caption: {result['generated_caption']}\")\n        print(\"--------\")\n\n    return results  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:57:27.268583Z","iopub.execute_input":"2024-12-18T15:57:27.269358Z","iopub.status.idle":"2024-12-18T15:57:27.276996Z","shell.execute_reply.started":"2024-12-18T15:57:27.269325Z","shell.execute_reply":"2024-12-18T15:57:27.276116Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"results = generate_and_save_captions_with_beam_search(\n    model, test_loader, vocab, device, beam_size=10, max_len=20, output_file=\"captions_output.txt\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:57:28.452551Z","iopub.execute_input":"2024-12-18T15:57:28.453396Z","iopub.status.idle":"2024-12-18T16:16:29.572412Z","shell.execute_reply.started":"2024-12-18T15:57:28.453362Z","shell.execute_reply":"2024-12-18T16:16:29.571509Z"}},"outputs":[{"name":"stdout","text":"Image: 1001773457_577c3a7d70.jpg\nActual Caption: a black dog and a spotted dog are fighting\nGenerated Caption: a black and white dog is playing with a brown dog .\n--------\nImage: 1001773457_577c3a7d70.jpg\nActual Caption: a black dog and a tri-colored dog playing with each other on the road .\nGenerated Caption: a black and white dog is playing with a brown dog .\n--------\nImage: 1001773457_577c3a7d70.jpg\nActual Caption: a black dog and a white dog with brown spots are staring at each other in the street\nGenerated Caption: a black and white dog is playing with a brown dog .\n--------\nImage: 1001773457_577c3a7d70.jpg\nActual Caption: two dogs of different breeds looking at each other on the road .\nGenerated Caption: a black and white dog is playing with a brown dog .\n--------\nImage: 1001773457_577c3a7d70.jpg\nActual Caption: two dogs on pavement moving toward each other .\nGenerated Caption: a black and white dog is playing with a brown dog .\n--------\nImage: 1009434119_febe49276a.jpg\nActual Caption: a black and white dog is running in a grassy garden surrounded by a white fence .\nGenerated Caption: a black and white dog running on grass .\n--------\nImage: 1009434119_febe49276a.jpg\nActual Caption: a black and white dog is running through the grass .\nGenerated Caption: a black and white dog running on grass .\n--------\nImage: 1009434119_febe49276a.jpg\nActual Caption: a boston terrier is running in the grass .\nGenerated Caption: a black and white dog running on grass .\n--------\nImage: 1009434119_febe49276a.jpg\nActual Caption: a boston terrier is running on lush green grass in front of a white fence .\nGenerated Caption: a black and white dog running on grass .\n--------\nImage: 1009434119_febe49276a.jpg\nActual Caption: a dog runs on the green grass near a wooden fence .\nGenerated Caption: a black and white dog running on grass .\n--------\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"actual_captions = [result['actual_caption'] for result in results]\ngenerated_captions = [result['generated_caption'] for result in results]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:19:46.795534Z","iopub.execute_input":"2024-12-18T16:19:46.796430Z","iopub.status.idle":"2024-12-18T16:19:46.803118Z","shell.execute_reply.started":"2024-12-18T16:19:46.796395Z","shell.execute_reply":"2024-12-18T16:19:46.802202Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"!pip install nltk\nimport nltk\nnltk.download('punkt')  # If needed\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\nsmooth = SmoothingFunction().method1\n\ndef clean_caption(caption_tokens):\n    return [w for w in caption_tokens if w not in ['<start>', '<end>', '<pad>']]\n\n# Clean up captions before BLEU calculation\nclean_actual = [clean_caption(ref) for ref in actual_captions]\nclean_generated = [clean_caption(hyp) for hyp in generated_captions]\n\nall_scores = []\nfor ref, hyp in zip(clean_actual, clean_generated):\n    reference = [ref]\n    hypothesis = hyp\n    score = sentence_bleu(reference, hypothesis, smoothing_function=smooth)\n    all_scores.append(score)\n\naverage_bleu = sum(all_scores) / len(all_scores)\nprint(f\"Average BLEU score (cleaned): {average_bleu}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:19:49.477677Z","iopub.execute_input":"2024-12-18T16:19:49.477998Z","iopub.status.idle":"2024-12-18T16:20:00.203655Z","shell.execute_reply.started":"2024-12-18T16:19:49.477972Z","shell.execute_reply":"2024-12-18T16:20:00.202664Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nAverage BLEU score (cleaned): 0.2802072441125255\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}